from flask import Flask, request, jsonify
from utils import search_articles, concatenate_content, generate_answer
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage

app = Flask(__name__)

# Initialize conversation memory
memory = ConversationBufferMemory(return_messages=True)

@app.route('/query', methods=['POST'])
def query():
    data = request.json
    user_query = data.get('query')

    # Step 1: Search and scrape articles based on the query
    articles = search_articles(user_query)

    # Step 2: Concatenate content from the scraped articles
    content = concatenate_content(articles)

    # Step 3: Generate an answer using the LLM

    # Return the jsonified text back to Streamlit



     #Step 3: Generate answer using Claude with memory
    conversation_history = memory.chat_memory.messages
    answer = generate_answer(content, query, conversation_history)
    print("Generated by Claude: --------------------->", answer)

    # Step 4: Update memory with the new interaction
    memory.chat_memory.add_user_message(user_query)
    memory.chat_memory.add_ai_message(answer)

    # Step 5: Return the content and conversation history as JSON
    return jsonify({
        "content": answer,
        "conversation": [
            {"role": "human" if isinstance(msg, HumanMessage) else "ai", "content": msg.content}
            for msg in memory.chat_memory.messages
        ]
    }), 200

if __name__ == '__main__':
    app.run(host='localhost', port=5001)